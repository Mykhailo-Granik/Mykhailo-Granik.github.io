
---
title: "PML_CourseProject"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data loading and preprocessing

First of all, lets load and save the training and testing data and load all necessary packages

```{r data loading}

library(caret)
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")

```


The, let's remove all of the columns with NA's in both data sets:

```{r data normalizing}

colNamesWithNaTraining = colnames(training)[colSums(is.na(training)) > 0]
colNamesWithNaTesting = colnames(testing)[colSums(is.na(testing)) > 0]
trainingReduced = training[, -which(names(training) %in% colNamesWithNaTraining)]
trainingReduced <- trainingReduced[, -which(names(trainingReduced) %in% colNamesWithNaTesting)]
testingReduced <- testing[, -which(names(testing) %in% colNamesWithNaTraining)]
testingReduced = testingReduced[, -which(names(testingReduced) %in% colNamesWithNaTesting)]

```

Then, let's divide training data set into training and validation subset. The validation subset is needed for choosing the better model (or choosing some meta parameters of the models).

It was decided to use only 10 percent of the data for training because of hardware reasons

```{r data patitioning}

inTrain = createDataPartition(trainingReduced$classe, p = 0.1)[[1]]
trainingSample = trainingReduced[inTrain,]
validationSample = trainingReduced[-inTrain,]

```

## Prediction

It was decided to use random forests algorithm with for training and making predictions. 

Couple of models were tried by hand and tested on the validation set (this process should be automized in future)
One of the best models is shown here:


```{r prediction1}
tr <- trainControl(method = "cv", number = 5)
modRf <- train(classe ~ roll_belt + pitch_belt + yaw_belt + total_accel_belt+roll_forearm, method = "rf", data = trainingSample, trControl = tr)
predRf = predict(modRf)
confusionMatrix(predRf, trainingSample$classe)$overall[1]
predRf = predict(modRf, newdata = validationSample)
confusionMatrix(predRf, validationSample$classe)$overall[1]

```

Received accuracy on the validation set is 87%. It's accuracy on the test set should be slightly less because we used validation set for model selection.

Since the training error is so small, it is possible that model still overfits. Probably, some regularization should be performed as well.

## Conclusions

What should I've done better:

1) Perform some model selection. That would've made the training faster and probably made the model simpler.

2) Train more models

3) Use model stacking or some other techniques of combining models for achieving better results

